# Module 3: Bellman Equations

At the end of this module

1. You will be able to cast RL problems as Markov Decision Processes (MDP)
2. You will be able to define value function and Q value function of the states in a MDP
3. You will be able to explain the contents of the Bellman equations and Bellman optimality theorems applied to MDPs
4. You will be able to perform value function updates and Q value function updates to determine the desirability of 
any state in the `CartPole-v0` environment, given a policy

## Plan

1. :movie_camera: Markov Decision Processes
2. :movie_camera: Value function and Q value function
3. :movie_camera: The Bellman equation and optimality theorem
4. :movie_camera: Value iteration
5. :pencil: **Exercise**: *Wrap the `CartPole-v0` environment to make it amenable to value iterations. Discretize observations to
first place of decimal.*
6. :pencil: **Exercise**: *Given a policy, find the values for a list of states.*

## References

1. [RL Course by David Silver, Lecture 2: Markov Decision Processes](https://www.youtube.com/watch?v=lfHX2hHRMVQ&t)
2. [RL Course by David Silver, Lecture 3: Planning by Dynamic Programming](https://www.youtube.com/watch?v=lfHX2hHRMVQ&t)

## Navigation

[Next - Module 4: GLIE Monte Carlo](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/04_glie_monte_carlo)

[Previous - Module 2: RL basics with `Open AI` Gym](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/02_rl_basics_with_openai_gym)
