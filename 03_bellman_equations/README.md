# Module 3: Bellman Equations

In this module, we will see Reinforcement Learning through the lens of Markov Decision Processes. We will also define value functions and 
Q value functions. 

With these ingredients, we will be able to explore the Bellman equations and Bellman optimatily theorems. We will see how one 
can iteratively solve a Reinforcement Learning problem with the help of the Bellman equations and theorems (value iteration and 
Q value iteration). This will be foundational in many Reinforcement Learning algorithms that we will learn later in the course. 

On the practical side, we will determine the value function for a toy policy in the CartPole-v0 environment using value 
iterations. 

# Required reading

Students enrolled in the course have access to lecture videos, coding screencasts and assignment solutions. You **do not** need to 
go through the reading material listed here, though it doesn't hurt if you are curious.

This section is for students who are financially disadvantaged and are not able to pay for the course. The reading materials are a 
collection of free online resources that cover many of the things that I address in the course videos (though not everything 
unfortunately). It's best to go through them in the order listed. 

After you are done studying the listed materials, try solving the assignments. If you can solve the assignments without problems, 
you have picked up the required knowledge for this module.

1. [RL Course by David Silver, Lecture 2: Markov Decision Processes](https://www.youtube.com/watch?v=lfHX2hHRMVQ&t)
2. [RL Course by David Silver, Lecture 3: Planning by Dynamic Programming](https://www.youtube.com/watch?v=lfHX2hHRMVQ&t)
