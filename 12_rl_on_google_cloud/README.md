# Module 12: RL on Google Cloud

At the end of this module

1. You will be able to decide if a GPU is going to substantially speed up your Deep RL code
2. You will be able to create a VM on Google Cloud Compute Engine and set up a RL environment with `OpenAI Gym` and `Keras` with 
`Tensorflow-GPU` backend
3. You will deploy the PPO algorithm you wrote earlier in Google Cloud :tada:
4. You will be able to benchmark and compare Deep RL algorithms execution on CPU and GPU

## Plan

1. :movie_camera: Neural Networks : CPU or GPU?
2. :movie_camera: Setting up a RL environment on `Google Cloud` with GPUs
3. :pencil: **Exercise**: *Run the PPO code on your computer's CPU and on a GPU instance on `Google Cloud` and compare speed of
execution in non reproducible mode and reproducible mode.*

## References

1. [Google Compute Engine Documentation](https://cloud.google.com/compute/docs/)
2. [Using a GPU & TensorFlow on Google Cloud Platform](https://medium.com/google-cloud/using-a-gpu-tensorflow-on-google-cloud-platform-1a2458f42b0)
3. [Tensorflow GPU support](https://www.tensorflow.org/install/gpu)
4. [Handle different versions of BLAS and LAPACK](https://wiki.debian.org/DebianScience/LinearAlgebraLibraries)

## Navigation 

[Next - Module 13: Deep Q Network](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/13_deep_q_network)

[Previous - Module 11: Proximal Policy Approximation](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/11_proximal_policy_optimization)


