#  Practical Reinforcement Learning - An online course

1. You will learn all the theory necessary for reasoning about Reinforcement Learning problems. You will be able to choose the right algorithm, tune performance, and debug like a pro. But I will not go into mathematical proofs and instead, point you to appropriate references, in case you fancy such things. The goal is to turn you into a Reinforcement Learning engineer as fast as possible and with no-fluff-and-nonsense.

2. My focus is teaching you to apply Reinforcement Learning in real world problems. Whenever we learn a new concept, we will code it up using `Python`, `OpenAI Gym` and `Keras`, so that you become familiar with all the practical implementation details. And believe me, nobody talks about these practical details! When I was implementing Reinforcement Learning algorithms for the first time, I searched and asked for practical details everywhere, but eventually had to learn it by making mistake after mistake after mistake.You don't have to.

3. You will learn best practices in Reinforcement Learning that will make everything seem easier. I have picked up 
these best practices by implementating tens of different RL algorithms on my own and find that they substantially
reduce implementation time and bugs.

4. You will implement state-of-the-art algorithms in Reinforcement Learning such as PPO and DQN. You will also develop the knowledge to understand and implement new research breakthroughs in this field. This is critical as this field is moving rapidly, and I can't teach you everything, even though I will update the course with new information regularly. But more 
importantly , if I can arm you with the tools to take on any Reinforcement Learning problem that you may face in your studies, career or projects on your own, then I think that is the best learning outcome.

Check out the course plan below. It's filled with informative videos and exercises. For each exercise, there's a coding 
screencast, where I systematically take you through the solution in case you can't solve it on your own. 

*The course is currently under preparation. As we speak, I am recording the videos, preparing Jupyter notebooks for the 
exercises and the screencasts. I expect to have the course ready in a few months time.* 


## Course Plan

[Module 1: RL Breakthroughs and Use Cases](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/01_rl_breakthroughs_and_use_cases)

[Module 2: RL basics with `Open AI` Gym](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/02_rl_basics_with_openai_gym)

[Module 3: Bellman Equations](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/03_bellman_equations)

[Module 4: GLIE Monte Carlo](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/04_glie_monte_carlo)

[Module 5: SARSA](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/05_sarsa)

[Module 6: Function approximation (Tile Coding)](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/06_fn_approx_tile_coding)

[Module 7: Function approximation (Fourier Series)](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/07_fn_approx_fourier_series)

[Module 8: Neural Network crash course with `Keras`](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/08_neural_network_crash_course_with_keras)

[Module 9: Function approximation (Neural Network)](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/09_fn_approx_neural_network)

[Module 10: Vanilla Policy Gradient](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/10_vanilla_policy_gradient)

[Module 11: Proximal Policy Approximation](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/11_proximal_policy_optimization)

[Module 12: RL On `Google Cloud`](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/12_rl_on_google_cloud)

[Module 13: Deep Q Network](https://github.com/gutfeeling/practical_rl_for_coders/tree/master/13_deep_q_network)
